{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a walkthrough of the Smoothie algorithm on synthetic data, including a simple mathematical derivation of the Smoothie weights. If interested in how to use Smoothie for model routing on real datasets, please see `tutorial.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set the following parameters:\n",
    "* embedding\\_size\n",
    "* m: number of models to route among\n",
    "* theta: size m vector where theta[i] ~ quality of ith model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 10\n",
    "m = 3 # number of prompts/models\n",
    "\n",
    "# construct groundtruth theta \"accuracies\"\n",
    "theta = np.random.random(m)*50 # all positive for now. Larger theta = higher quality model generations.\n",
    "print(\"Theta:\", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct mean vector (length m x embedding\\_size) $\\mu$ = 0 and covariance matrix $\\Sigma$ = diag(1/2 theta). \n",
    "These are the parameters of the multivariate Gaussian corresponding to each model's error in embedding space,\n",
    "where error is defined as the vector difference between a model's generation and the unknown ground-truth generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: [ 7.51705954 39.08450852 49.46922858]\n",
      "Covariance matrix: [0.06651537 0.06651537 0.06651537 0.06651537 0.06651537 0.06651537\n",
      " 0.06651537 0.06651537 0.06651537 0.06651537 0.01279279 0.01279279\n",
      " 0.01279279 0.01279279 0.01279279 0.01279279 0.01279279 0.01279279\n",
      " 0.01279279 0.01279279 0.01010729 0.01010729 0.01010729 0.01010729\n",
      " 0.01010729 0.01010729 0.01010729 0.01010729 0.01010729 0.01010729]\n",
      "Mean: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# construct mu and sigma for multivariate gaussian formulation of the model.\n",
    "sigma_diag = np.zeros(m*embedding_size)\n",
    "for i in range(len(sigma_diag)):\n",
    "    prompt_idx = int(i / embedding_size)\n",
    "    sigma_diag[i] = 1/(2 * theta[prompt_idx])\n",
    "\n",
    "print(\"Covariance matrix diagonal:\", sigma_diag)\n",
    "\n",
    "sigma = np.diag(sigma_diag)\n",
    "mu = np.zeros(m * embedding_size)\n",
    "\n",
    "print(\"Mean:\", mu) # Zero mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate our data. Our sampling process is:\n",
    "1) Sample from $\\mathcal{N}(\\mu, \\Sigma)$, reshape this into a set of m error vectors of length embedding\\_size.\n",
    "2) Randomly sample an embedding vector corresponding to the ground-truth generation.\n",
    "3) We add each error vector to the ground-truth embedding to get each model generation's embedding. \n",
    "\n",
    "We end with a tensor all\\_lfs\\_y, which contains all m+1 embeddings per sample. In practice, we have access to the first m entries of each sample, which correspond to the model generations' embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10000 # number of samples\n",
    "all_lfs_y = []\n",
    "all_diffs = []\n",
    "count = 0\n",
    "while count < n:\n",
    "    # Construct embeddings by sampling 1) a multivariate gaussian \"diff\" 2) a ground truth y 3) adding them together\n",
    "    # and setting LF = diff + y\n",
    "    diff = np.random.multivariate_normal(mu, sigma)\n",
    "    all_diffs.append(diff)\n",
    "\n",
    "    y = np.random.random(embedding_size)\n",
    "    y_repeated = np.tile(y, reps= m)\n",
    "\n",
    "    lfs = (y_repeated + diff).reshape((m, embedding_size))\n",
    "    lfs_y = np.concatenate([lfs, y.reshape((-1, embedding_size))], axis=0)\n",
    "\n",
    "    all_lfs_y.append(lfs_y)\n",
    "    count += 1\n",
    "\n",
    "all_lfs_y = np.array(all_lfs_y)\n",
    "all_lfs_y.shape # The first three rows of the second dimension corresponds to model embeddings, and the fourth corresponds to y embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all\\_lfs\\_y, we can use Smoothie to estimate theta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We describe the math behind Smoothie. Let $x$ be the input sample and let $\\lambda_i(x)$ be the embedding of the generation for $x$ produced by the $i$th model. Let $y(x)$ be the embedding of the true optimal generation. Then we can express the fact that error embedding vectors are Gaussian as $[\\lambda_1(x) - y(x), \\dots, \\lambda_m(x) - y(x)] \\sim \\mathcal{N}(\\mu, \\Sigma)$. The following holds:\n",
    "\n",
    "$\\begin{align}\\mathbb{E}[\\|\\lambda_i(x) - \\lambda_j(x)\\|^2] &= \\mathbb{E}[\\|(\\lambda_i(x) - y(x)) - (\\lambda_j(x) - y(x))\\|^2] \\nonumber \\\\\n",
    "&= \\mathbb{E}[\\|\\lambda_i(x) - y(x) \\|^2] + \\mathbb{E}[\\|\\lambda_j(x) - y(x) \\|^2] - 2\\mathbb{E}[(\\lambda_i(x) - y(x))^\\top (\\lambda_j(x) - y(x))] \\nonumber \\end{align}$\n",
    "\n",
    "Since $\\Sigma$ is a diagonal matrix, the $2\\mathbb{E}[(\\lambda_i(x) - y(x))^\\top (\\lambda_j(x) - y(x))]$ term is $0$, and therefore we have an elegant decomposition:\n",
    "$\\begin{align}\\mathbb{E}[\\|\\lambda_i(x) - \\lambda_j(x)\\|^2] = \\mathbb{E}[\\|\\lambda_i(x) - y(x) \\|^2] + \\mathbb{E}[\\|\\lambda_j(x) - y(x) \\|^2] \\nonumber \\end{align}$\n",
    "\n",
    "We write this equation for $\\lambda_j, \\lambda_k$ and $\\lambda_i, \\lambda_k$ to get a system of three equations:\n",
    "$\\begin{align}\\mathbb{E}[\\|\\lambda_i(x) - \\lambda_j(x)\\|^2] &= \\mathbb{E}[\\|\\lambda_i(x) - y(x) \\|^2] + \\mathbb{E}[\\|\\lambda_j(x) - y(x) \\|^2] \\nonumber \\\\\n",
    "\\mathbb{E}[\\|\\lambda_j(x) - \\lambda_k(x)\\|^2] &= \\mathbb{E}[\\|\\lambda_j(x) - y(x) \\|^2] + \\mathbb{E}[\\|\\lambda_k(x) - y(x) \\|^2] \\nonumber \\\\\n",
    "\\mathbb{E}[\\|\\lambda_i(x) - \\lambda_k(x)\\|^2] &= \\mathbb{E}[\\|\\lambda_i(x) - y(x) \\|^2] + \\mathbb{E}[\\|\\lambda_k(x) - y(x) \\|^2] \\nonumber \\end{align}$\n",
    "\n",
    "There are three unknown quantities, the average L2 norm of the $i, j, k$ th error vectors (RHS), and three observable qualities, the average L2 norm of the difference between pairs of $i, j, k$ embeddings (LHS). Solving this system of equations, we have\n",
    "$\\begin{align}\\mathbb{E}[\\|\\lambda_i(x) - y(x) \\|^2] &= \\frac{1}{2} \\big(\\mathbb{E}[\\|\\lambda_i(x) - \\lambda_j(x)\\|^2] + \\mathbb{E}[\\|\\lambda_i(x) - \\lambda_k(x)\\|^2] - \\mathbb{E}[\\|\\lambda_j(x) - \\lambda_k(x) \\|^2] \\big) \\end{align}$\n",
    "\n",
    "and similarly for $\\mathbb{E}[\\|\\lambda_j(x) - y(x) \\|^2]$ and $\\mathbb{E}[\\|\\lambda_k(x) - y(x) \\|^2]$. \n",
    "\n",
    "Some simple algebra lets us recover $\\theta_i = \\frac{embedding\\_size}{2\\mathbb{E}[\\|\\lambda_i(x) - y(x) \\|^2]}$, which are the final *Smoothie Weights*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet(i, j, k):\n",
    "    # Computes an estimate of E[||lambda_i(x) - y(x)||^2]\n",
    "    diff_ij = (np.linalg.norm(all_lfs_y[:, i, :] - all_lfs_y[:, j, :], axis=1, ord=2)**2).mean()\n",
    "    diff_ik = (np.linalg.norm(all_lfs_y[:, i, :] - all_lfs_y[:, k, :], axis=1, ord=2)**2).mean()\n",
    "    diff_jk = (np.linalg.norm(all_lfs_y[:, j, :] - all_lfs_y[:, k, :], axis=1, ord=2)**2).mean()\n",
    "\n",
    "    return 0.5*(diff_ij + diff_ik - diff_jk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23288287180858308 0.23369112730365055\n",
      "0.5578053959351905 0.554497767481926\n",
      "0.1796984318051682 0.18107175815446175\n"
     ]
    }
   ],
   "source": [
    "diff = np.zeros(m)\n",
    "\n",
    "for i in range(m):\n",
    "    other_idxs = np.delete(np.arange(m), i)\n",
    "    j, k = np.random.choice(other_idxs, size=2, replace=False)\n",
    "    diff[i] = triplet(i, j, k)\n",
    "\n",
    "    # compare to true value\n",
    "    print(diff[i], (np.linalg.norm(all_lfs_y[:, i, :] - all_lfs_y[:, m, :], axis=1)**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to canonical parameters, i.e., Smoothie Weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.47002036  8.96369959 27.82439418] [21.44408149  9.07740938 27.70640861]\n"
     ]
    }
   ],
   "source": [
    "# convert mean parameters to canonical parameters \n",
    "theta_estimate = embedding_size/(2*diff)\n",
    "\n",
    "print(theta_estimate, theta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
